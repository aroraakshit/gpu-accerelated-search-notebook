{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Gave Me a \\$15K Data Science Workstation - here's what I did with it\n",
    "### Creating a GPU Accelerated Pubmed Search Engine\n",
    "This notebook is an adaptation of my Towards Data Science Article availabe [here](https://towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35).\n",
    "\n",
    "![img1](https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/deep-learning-solutions/data-science/data-science-laptop-workstation-4c25-p@2x.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download and Process XML Data\n",
    "This step gives you example data to work with for the tutorial. For the actual post, I worked with all of Pubmed. However, for the sake of brevity here I use the abstracts from a single file.\n",
    "\n",
    "Here, I walked through the example with one Pubmed file, although you could repeat this process for every file in the directory. I explicitely chose a document that has newer abstracts. Make sure your computer has enough processing power to handle this part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "--2020-03-11 09:49:04--  https://mbr.nlm.nih.gov/Download/Baselines/2019/pubmed19n0972.xml.gz\n",
      "Resolving mbr.nlm.nih.gov (mbr.nlm.nih.gov)... 130.14.53.15\n",
      "Connecting to mbr.nlm.nih.gov (mbr.nlm.nih.gov)|130.14.53.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8755134 (8.3M) [application/x-gzip]\n",
      "Saving to: ‘pubmed19n0972.xml.gz’\n",
      "\n",
      "pubmed19n0972.xml.g 100%[===================>]   8.35M  5.36MB/s    in 1.6s    \n",
      "\n",
      "2020-03-11 09:49:07 (5.36 MB/s) - ‘pubmed19n0972.xml.gz’ saved [8755134/8755134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data dir\n",
    "!mkdir data\n",
    "# download single Pubmed XML from the directory\n",
    "!wget https://mbr.nlm.nih.gov/Download/Baselines/2019/pubmed19n0972.xml.gz \n",
    "!mv pubmed19n0972.xml.gz  data/pubmed-data.xml.gz\n",
    "# unzip it\n",
    "!gunzip data/pubmed-data.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the XML to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_file_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def get_pubmed_articles(text, field=\"PubmedArticle\"):\n",
    "    soup = BeautifulSoup(text,\"xml\")\n",
    "    documents = soup.find_all(field)\n",
    "    return documents\n",
    "\n",
    "def get_pubmed_article_fields(soup, fields=[\"AbstractText\",\"Year\"]):\n",
    "    d = {}\n",
    "    for f in fields:\n",
    "        item = '' if soup.find(f) is None else soup.find(f).text\n",
    "        d[f] = item\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PubmedArticle>\n",
      "<MedlineCitation Owner=\"NLM\" Status=\"Publisher\">\n",
      "<PMID Version=\"1\">30516271</PMID>\n",
      "<DateRevised>\n",
      "<Year>2018</Year>\n",
      "<Month>12</Month>\n",
      "<Day>05</Day>\n",
      "</DateRevised>\n",
      "<Article PubModel=\"Print-Electronic\">\n",
      "<Journal>\n",
      "<ISSN IssnType=\"Print\">0012-9658</ISSN>\n",
      "<JournalIssue CitedMedium=\"Print\">\n",
      "<PubDate>\n",
      "<Year>2018</Year>\n",
      "<Month>Dec</Month>\n",
      "<Day>05</Day>\n",
      "</PubDate>\n",
      "</JournalIssue>\n",
      "<Title>Ecology</Title>\n",
      "<ISOAbbreviation>Ecology</ISOAbbreviation>\n",
      "</Journal>\n",
      "<ArticleTitle>Spatial scale modulates the inference of metacommunity assembly processes.</ArticleTitle>\n",
      "<ELocationID EIdType=\"doi\" ValidYN=\"Y\">10.1002/ecy.2576</ELocationID>\n",
      "<Abstract>\n",
      "<AbstractText>The abundance and distribution of species across the landscape depend on the interaction between local, spatial and stochastic processes. However, empirical syntheses relating these processes to spatio-temporal patterns of structure in metacommunities remains elusive. One important reason for this lack of synthesis is that the relative importance of the core assembly processes (dispersal, selection and drift) critically depends on the spatial grain and extent over which communities are studied. To illustrate this, we simulated different aspects of community assembly on heterogeneous landscapes, including the strength of response to environmental heterogeneity (inherent to niche theory) versus dispersal and stochastic drift (inherent to neutral theory). We show that increasing spatial extent leads to increasing importance of niche selection, whereas increasing spatial grain leads to decreasing importance of niche selection. The strength of these scaling effects depended on environment configuration, dispersal capacity and niche breadth. By mapping the variation observed from the scaling effects in simulations, we could recreate the entire range of variation observed within and among empirical studies. This means that variation in the relative importance of assembly processes among empirical studies is largely scale dependent and cannot be directly compared. The scaling coefficient of the relative contribution of assembly processes, however, can be interpreted as a scale-integrative estimate to compare assembly processes across different regions and ecosystems. This emphasizes the necessity to consider spatial scaling as an explicit component of studies intended to infer the importance of community assembly processes. This article is protected by copyright. All rights reserved.</AbstractText>\n",
      "<CopyrightInformation>This article is protected by copyright. All rights reserved.</CopyrightInformation>\n",
      "</Abstract>\n",
      "<AuthorList CompleteYN=\"Y\">\n",
      "<Author ValidYN=\"Y\">\n",
      "<LastName>Viana</LastName>\n",
      "<ForeName>Duarte S</ForeName>\n",
      "<Initials>DS</Initials>\n",
      "<AffiliationInfo>\n",
      "<Affiliation>German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Germany.</Affiliation>\n",
      "</AffiliationInfo>\n",
      "</Author>\n",
      "<Author ValidYN=\"Y\">\n",
      "<LastName>Chase</LastName>\n",
      "<ForeName>Jonathan M</ForeName>\n",
      "<Initials>JM</Initials>\n",
      "<AffiliationInfo>\n",
      "<Affiliation>German Centre for Integrative Biodiversity Research (iDiv) Halle-Jena-Leipzig, Germany.</Affiliation>\n",
      "</AffiliationInfo>\n",
      "<AffiliationInfo>\n",
      "<Affiliation>Institute for Computer Science, Martin Luther University Halle-Wittenberg, Halle (Saale), Germany.</Affiliation>\n",
      "</AffiliationInfo>\n",
      "</Author>\n",
      "</AuthorList>\n",
      "<Language>eng</Language>\n",
      "<PublicationTypeList>\n",
      "<PublicationType UI=\"D016428\">Journal Article</PublicationType>\n",
      "</PublicationTypeList>\n",
      "<ArticleDate DateType=\"Electronic\">\n",
      "<Year>2018</Year>\n",
      "<Month>12</Month>\n",
      "<Day>05</Day>\n",
      "</ArticleDate>\n",
      "</Article>\n",
      "<MedlineJournalInfo>\n",
      "<Country>United States</Country>\n",
      "<MedlineTA>Ecology</MedlineTA>\n",
      "<NlmUniqueID>0043541</NlmUniqueID>\n",
      "<ISSNLinking>0012-9658</ISSNLinking>\n",
      "</MedlineJournalInfo>\n",
      "<KeywordList Owner=\"NOTNLM\">\n",
      "<Keyword MajorTopicYN=\"N\">Community assembly</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Dispersal</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Ecological drift</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Metacommunity</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Neutral theory</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Niche selection</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Sampling grain</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Spatial extent</Keyword>\n",
      "<Keyword MajorTopicYN=\"N\">Spatial scale</Keyword>\n",
      "</KeywordList>\n",
      "</MedlineCitation>\n",
      "<PubmedData>\n",
      "<History>\n",
      "<PubMedPubDate PubStatus=\"entrez\">\n",
      "<Year>2018</Year>\n",
      "<Month>12</Month>\n",
      "<Day>6</Day>\n",
      "<Hour>6</Hour>\n",
      "<Minute>0</Minute>\n",
      "</PubMedPubDate>\n",
      "<PubMedPubDate PubStatus=\"pubmed\">\n",
      "<Year>2018</Year>\n",
      "<Month>12</Month>\n",
      "<Day>6</Day>\n",
      "<Hour>6</Hour>\n",
      "<Minute>0</Minute>\n",
      "</PubMedPubDate>\n",
      "<PubMedPubDate PubStatus=\"medline\">\n",
      "<Year>2018</Year>\n",
      "<Month>12</Month>\n",
      "<Day>6</Day>\n",
      "<Hour>6</Hour>\n",
      "<Minute>0</Minute>\n",
      "</PubMedPubDate>\n",
      "</History>\n",
      "<PublicationStatus>aheadofprint</PublicationStatus>\n",
      "<ArticleIdList>\n",
      "<ArticleId IdType=\"pubmed\">30516271</ArticleId>\n",
      "<ArticleId IdType=\"doi\">10.1002/ecy.2576</ArticleId>\n",
      "</ArticleIdList>\n",
      "</PubmedData>\n",
      "</PubmedArticle>\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA = \"data/pubmed-data.xml\"\n",
    "\n",
    "text = get_file_text(RAW_DATA)\n",
    "documents = get_pubmed_articles(text)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the `AbstractText` and `Year` fields from each of these xml objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AbstractText</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The abundance and distribution of species acro...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Several recent methods address the dimension r...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Research on regime shifts has focused primaril...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The diversity-invasibility hypothesis and ecol...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Most studies consider aboveground plant specie...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        AbstractText  Year\n",
       "0  The abundance and distribution of species acro...  2018\n",
       "1  Several recent methods address the dimension r...  2018\n",
       "2  Research on regime shifts has focused primaril...  2018\n",
       "3  The diversity-invasibility hypothesis and ecol...  2018\n",
       "4  Most studies consider aboveground plant specie...  2018"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis = []\n",
    "for doc in documents:\n",
    "    fields = get_pubmed_article_fields(doc)\n",
    "    lis.append(fields)\n",
    "    \n",
    "df = pd.DataFrame(lis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more intensive and particular way to process the documents that can lead to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/pubmed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Accelerated Data Load and Processing with NVIDIA RAPIDs and Dask\n",
    "\n",
    "![img2](https://devblogs.nvidia.com/wp-content/uploads/2018/10/pipeline-1024x382.png)\n",
    "\n",
    "The first thing we can do is read in the data using dask. The wildcard matching makes it super easy to read a ton of csv files in a directory utilizing all the GPUs on your system. To monitor these GPUs, use `watch -n 0.5 nvidia-smi` in another terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dask_cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-69bd3fa80d5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdask_cuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocalCUDACluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdask_cudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dask_cuda'"
     ]
    }
   ],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':  # need this for the cluster\n",
    "    cluster = LocalCUDACluster()  # runs on two local GPUs\n",
    "    client = Client(cluster)\n",
    "    t0 = time.time()\n",
    "    gdf = dask_cudf.read_csv('data/*.csv') # read all csv files\n",
    "    abstract = gdf.Abstract.compute()\n",
    "    t1 = time.time()\n",
    "    \n",
    "print(\"Read %s abstract in %f seconds\" % (len(gdf), t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is extremely powerful, however there is more pandas-like funcitonality in cudf. Using simple Python code and only one of our GPUs, here we:\n",
    "\n",
    "- read in every csv in our data dir\n",
    "- lowercase all strings in the Abstract column\n",
    "- remove all punctutation\n",
    "\n",
    "This data cleaning operation could almost certainly be improved upon for greater efficiency. However, for our purposes here it serves the use case well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-785ddc2dc2c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcudf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cudf'"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "PATH = \"data/\"\n",
    "COLUMN = \"AbstractText\"\n",
    "\n",
    "start = time.time()\n",
    "i = 0\n",
    "\n",
    "for f in os.listdir(PATH):\n",
    "    t0 = time.time()\n",
    "    df = cudf.read_csv(PATH + f) # read using cudf instead of pandas\n",
    "    length = len(df.dropna(subset=[COLUMN]))\n",
    "    df[COLUMN] = df[COLUMN].str.lower()\n",
    "    df[COLUMN] = df[COLUMN].str.translate(str.maketrans('','',string.punctuation))\n",
    "    t1 = time.time()\n",
    "    print(\"Processed %i abstracts in %s seconds\" % (length, t1-t0))\n",
    "    i += 1\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed %i files in %s seconds\" % (i, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Information Retreival \n",
    "\n",
    "As context for how to approach the information retreival problem, here is a basic search class using sklearn. This is CPU bound, and uses cosine similarity on the vectors to find similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "class PubmedTfidfTrainServe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = None\n",
    "        self.vectorizer_name = \"vectorizer.pickle\"\n",
    "        self.lowercase = True\n",
    "        self.text_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_data(text):\n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self, path, column):\n",
    "        df = pd.read_csv(path)\n",
    "        self.text_data = df[column]\n",
    "        self.text_data = [self.preprocess_data(x) for x in self.text_data]\n",
    "\n",
    "    def train(self):\n",
    "        self.vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=self.lowercase)  \n",
    "        self.tfidf = self.vectorizer.fit_transform(self.text_data)\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        pickle.dump(self.vectorizer, open(self.vectorizer_name, \"wb\"))\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.vectorizer = pickle.load(open(model_path, \"rb\"))\n",
    "\n",
    "    def search(self, text, n_results=10):\n",
    "\n",
    "        if not self.vectorizer:\n",
    "            self.load_model()\n",
    "\n",
    "        vector = self.vectorizer.transform([str(text)])\n",
    "        cosine_similarities = linear_kernel(vector, self.tfidf).flatten()\n",
    "        related_doc_indicies = cosine_similarities.argsort()[:-n_results:-1]\n",
    "        return [self.text_data[i] for i in related_doc_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at some point, these same APIs shold be available via cuml in RAPIDs and we can run both training and inference on GPUs. However, at the moment cuml does not cover all of the features in sklearn. Below is a hacky way to read the data and train the vectorizer on CPU, then do the inference itself on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "search_term = \"heart\"\n",
    "\n",
    "pubmedTfidf = PubmedTfidfTrainServe()\n",
    "pubmedTfidf.load_data(\"data/pubmed.csv\",\"AbstractText\")\n",
    "pubmedTfidf.train()\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "def create_search_vector(search_term):\n",
    "    array = pubmedTfidf.vectorizer.transform([search_term]).toarray()\n",
    "    tensor = tf.convert_to_tensor(array)\n",
    "    return tensor \n",
    "\n",
    "## convert the sparse matrix to a tensor\n",
    "tfidf = convert_sparse_matrix_to_sparse_tensor(pubmedTfidf.tfidf)\n",
    "## vectorize a search term and do the same \n",
    "search_vector = create_search_vector(search_term)\n",
    "## perform the matrix multiplication \n",
    "res = tf.sparse.sparse_dense_matmul(\n",
    "    tfidf,\n",
    "    tf.transpose(search_vector),\n",
    "    adjoint_a=False,\n",
    "    adjoint_b=False,\n",
    "    name=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To santiy check our results, let's do one search with sklearn and one with GPU accelerated tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relationship between prognosis and heart rate remains unclear among patients diagnosed with heart failure with midrange ejection fraction hfmref the aim of the present study was to assess the effect of heart rate in this group of patients\n"
     ]
    }
   ],
   "source": [
    "print(pubmedTfidf.search(\"heart\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relationship between prognosis and heart rate remains unclear among patients diagnosed with heart failure with midrange ejection fraction hfmref the aim of the present study was to assess the effect of heart rate in this group of patients\n"
     ]
    }
   ],
   "source": [
    "x = tf.math.argmax(res)\n",
    "tf.keras.backend.eval(x)[0]\n",
    "print(pubmedTfidf.text_data[4128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tight, they both work - but since tensorflow is highly optimized for GPU inference should be faster - especially at scale. However, TF-IDF vectors quickly get massive since we have one position for every unique word or token. \n",
    "\n",
    "Fortunately, with the advances of NLP we can represent text contextually in significantly less dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Accelerated Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF fails with lots of data due to issues related with memory (with the naive approach used above). Using novel NLP models like BERT, we can make our information retrieval both faster and contextual.\n",
    "\n",
    "Let's start by downloading one of the BERT large models and unzipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-03 16:54:18--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.8.16, 2607:f8b0:4004:803::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.8.16|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1248381879 (1.2G) [application/zip]\n",
      "Saving to: ‘wwm_uncased_L-24_H-1024_A-16.zip’\n",
      "\n",
      "100%[====================================>] 1,248,381,879  225MB/s   in 6.2s   \n",
      "\n",
      "2020-03-03 16:54:24 (191 MB/s) - ‘wwm_uncased_L-24_H-1024_A-16.zip’ saved [1248381879/1248381879]\n",
      "\n",
      "Archive:  wwm_uncased_L-24_H-1024_A-16.zip\n",
      "   creating: wwm_uncased_L-24_H-1024_A-16/\n",
      "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
      "  inflating: wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
      "wwm_uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001:  write error (disk full?).  Continue? (y/n/^C) ^C\n"
     ]
    }
   ],
   "source": [
    "# download model\n",
    "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "# unzip model\n",
    "!unzip wwm_uncased_L-24_H-1024_A-16.zip\n",
    "# start service with both GPUs available - do this another terminal\n",
    "# bert-serving-start -model_dir wwm_uncased_L-24_H-1024_A-16 -num_worker=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import faiss\n",
    "import pickle\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "DATA_PATH = '/data/'\n",
    "d = 2014\n",
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use faiss to create the GPU accelerated index. Here, we first create a CPU index and then convert it to the GPU counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpus = faiss.get_num_gpus()\n",
    "print(\"number of GPUs:\", ngpus)\n",
    "cpu_index = faiss.IndexFlatL2(d)\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n",
    "  cpu_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all the touting of GPU parallelization and acceleration, I write a for loop and just process the content with brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for f in os.listdir(DATA_PATH):\n",
    "    ## read data\n",
    "    df = cudf.read_csv(DATA_PATH+f)\n",
    "    n_abstracts = len(df)\n",
    "    text.append(df['AbstractText'])\n",
    "    \n",
    "\n",
    "    ## vectorize and index in two lines \n",
    "    vectors = bc.encode(list(df['Abstract']))\n",
    "    gpu_index.add(vectors.astype('float32'))\n",
    "    print(\"Total vectors: \", gpu_index.ntotal)\n",
    "    print(\"Total abstracts: \", len(text))\n",
    "\n",
    "    ## save index to disk if desired\n",
    "    cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "    faiss.write_index(cpu_index,\"data/vector.index\")\n",
    "    print(\"Saved index to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've read the text data in, vectorized it, and passed those vectors to our fast index. All that's left is to confirm and sanity check our search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "search_vector = bc.encode([\"this is a search query about the brain and blood\"])\n",
    "distances,indicies = index.search(search_vector.astype('float32'), k=3)\n",
    "t1 = time.time()\n",
    "print(\"Search query ran in %f seconds\" % t1-t0)\n",
    "print([text[i] for i in indicies])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
