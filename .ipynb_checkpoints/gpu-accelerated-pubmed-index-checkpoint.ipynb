{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA Gave Me a \\$15K Data Science Workstation - here's what I did with it\n",
    "### Creating a GPU Accelerated Pubmed Search Engine\n",
    "This notebook is an adaptation of my Towards Data Science Article availabe [here](https://towardsdatascience.com/nvidia-gave-me-a-15k-data-science-workstation-heres-what-i-did-with-it-70cfb069fc35).\n",
    "\n",
    "![img1](https://www.nvidia.com/content/dam/en-zz/Solutions/deep-learning/deep-learning-solutions/data-science/data-science-laptop-workstation-4c25-p@2x.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download and Process XML Data\n",
    "This step gives you example data to work with for the tutorial. For the actual post, I worked with all of Pubmed. However, for the sake of brevity here I use the abstracts from a single file.\n",
    "\n",
    "Here, I walked through the example with one Pubmed file, although you could repeat this process for every file in the directory. I explicitely chose a document that has newer abstracts. Make sure your computer has enough processing power to handle this part of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dir\n",
    "!mkdir data\n",
    "# download single Pubmed XML from the directory\n",
    "!wget https://mbr.nlm.nih.gov/Download/Baselines/2019/pubmed19n0972.xml.gz \n",
    "!mv pubmed19n0972.xml.gz  data/pubmed-data.xml.gz\n",
    "# unzip it\n",
    "!gunzip data/pubmed-data.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the XML to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_file_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def get_pubmed_articles(text, field=\"PubmedArticle\"):\n",
    "    soup = BeautifulSoup(text,\"xml\")\n",
    "    documents = soup.find_all(field)\n",
    "    return documents\n",
    "\n",
    "def get_pubmed_article_fields(soup, fields=[\"AbstractText\",\"Year\"]):\n",
    "    d = {}\n",
    "    for f in fields:\n",
    "        item = '' if soup.find(f) is None else soup.find(f).text\n",
    "        d[f] = item\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RAW_DATA = \"data/pubmed-data.xml\"\n",
    "\n",
    "text = get_file_text(RAW_DATA)\n",
    "documents = get_pubmed_articles(text)\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the `AbstractText` and `Year` fields from each of these xml objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = []\n",
    "for doc in documents:\n",
    "    fields = get_pubmed_article_fields(doc)\n",
    "    lis.append(fields)\n",
    "    \n",
    "df = pd.DataFrame(lis)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a more intensive and particular way to process the documents that can lead to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/pubmed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Accelerated Data Load and Processing with NVIDIA RAPIDs and Dask\n",
    "\n",
    "![img2](https://devblogs.nvidia.com/wp-content/uploads/2018/10/pipeline-1024x382.png)\n",
    "\n",
    "The first thing we can do is read in the data using dask. The wildcard matching makes it super easy to read a ton of csv files in a directory utilizing all the GPUs on your system. To monitor these GPUs, use `watch -n 0.5 nvidia-smi` in another terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "import dask_cudf\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "if __name__ == '__main__':  # need this for the cluster\n",
    "    cluster = LocalCUDACluster()  # runs on two local GPUs\n",
    "    client = Client(cluster)\n",
    "    t0 = time.time()\n",
    "    gdf = dask_cudf.read_csv('data/*.csv') # read all csv files\n",
    "    abstract = gdf.Abstract.compute()\n",
    "    t1 = time.time()\n",
    "    \n",
    "print(\"Read %s abstract in %f seconds\" % (len(gdf), t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is extremely powerful, however there is more pandas-like funcitonality in cudf. Using simple Python code and only one of our GPUs, here we:\n",
    "\n",
    "- read in every csv in our data dir\n",
    "- lowercase all strings in the Abstract column\n",
    "- remove all punctutation\n",
    "\n",
    "This data cleaning operation could almost certainly be improved upon for greater efficiency. However, for our purposes here it serves the use case well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "\n",
    "PATH = \"data/\"\n",
    "COLUMN = \"AbstractText\"\n",
    "\n",
    "start = time.time()\n",
    "i = 0\n",
    "\n",
    "for f in os.listdir(PATH):\n",
    "    t0 = time.time()\n",
    "    df = cudf.read_csv(PATH + f) # read using cudf instead of pandas\n",
    "    length = len(df.dropna(subset=[COLUMN]))\n",
    "    df[COLUMN] = df[COLUMN].str.lower()\n",
    "    df[COLUMN] = df[COLUMN].str.translate(str.maketrans('','',string.punctuation))\n",
    "    t1 = time.time()\n",
    "    print(\"Processed %i abstracts in %s seconds\" % (length, t1-t0))\n",
    "    i += 1\n",
    "\n",
    "end = time.time()\n",
    "print(\"Processed %i files in %s seconds\" % (i, end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Information Retreival \n",
    "\n",
    "As context for how to approach the information retreival problem, here is a basic search class using sklearn. This is CPU bound, and uses cosine similarity on the vectors to find similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import sklearn.feature_extraction\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "class PubmedTfidfTrainServe:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vectorizer = None\n",
    "        self.vectorizer_name = \"vectorizer.pickle\"\n",
    "        self.lowercase = True\n",
    "        self.text_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_data(text):\n",
    "        text = str(text).lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return text.strip()\n",
    "\n",
    "    def load_data(self, path, column):\n",
    "        df = pd.read_csv(path)\n",
    "        self.text_data = df[column]\n",
    "        self.text_data = [self.preprocess_data(x) for x in self.text_data]\n",
    "\n",
    "    def train(self):\n",
    "        self.vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=self.lowercase)  \n",
    "        self.tfidf = self.vectorizer.fit_transform(self.text_data)\n",
    "        self.save_model()\n",
    "\n",
    "    def save_model(self):\n",
    "        pickle.dump(self.vectorizer, open(self.vectorizer_name, \"wb\"))\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.vectorizer = pickle.load(open(model_path, \"rb\"))\n",
    "\n",
    "    def search(self, text, n_results=10):\n",
    "\n",
    "        if not self.vectorizer:\n",
    "            self.load_model()\n",
    "\n",
    "        vector = self.vectorizer.transform([str(text)])\n",
    "        cosine_similarities = linear_kernel(vector, self.tfidf).flatten()\n",
    "        related_doc_indicies = cosine_similarities.argsort()[:-n_results:-1]\n",
    "        return [self.text_data[i] for i in related_doc_indicies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at some point, these same APIs shold be available via cuml in RAPIDs and we can run both training and inference on GPUs. However, at the moment cuml does not cover all of the features in sklearn. Below is a hacky way to read the data and train the vectorizer on CPU, then do the inference itself on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "search_term = \"heart\"\n",
    "\n",
    "pubmedTfidf = PubmedTfidfTrainServe()\n",
    "pubmedTfidf.load_data(\"data/pubmed.csv\",\"AbstractText\")\n",
    "pubmedTfidf.train()\n",
    "\n",
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)\n",
    "\n",
    "def create_search_vector(search_term):\n",
    "    array = pubmedTfidf.vectorizer.transform([search_term]).toarray()\n",
    "    tensor = tf.convert_to_tensor(array)\n",
    "    return tensor \n",
    "\n",
    "## convert the sparse matrix to a tensor\n",
    "tfidf = convert_sparse_matrix_to_sparse_tensor(pubmedTfidf.tfidf)\n",
    "## vectorize a search term and do the same \n",
    "search_vector = create_search_vector(search_term)\n",
    "## perform the matrix multiplication \n",
    "res = tf.sparse.sparse_dense_matmul(\n",
    "    tfidf,\n",
    "    tf.transpose(search_vector),\n",
    "    adjoint_a=False,\n",
    "    adjoint_b=False,\n",
    "    name=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To santiy check our results, let's do one search with sklearn and one with GPU accelerated tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pubmedTfidf.search(\"heart\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.math.argmax(res)\n",
    "tf.keras.backend.eval(x)[0]\n",
    "print(pubmedTfidf.text_data[4128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tight, they both work - but since tensorflow is highly optimized for GPU inference should be faster - especially at scale. However, TF-IDF vectors quickly get massive since we have one position for every unique word or token. \n",
    "\n",
    "Fortunately, with the advances of NLP we can represent text contextually in significantly less dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU Accelerated Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF fails with lots of data due to issues related with memory (with the naive approach used above). Using novel NLP models like BERT, we can make our information retrieval both faster and contextual.\n",
    "\n",
    "Let's start by downloading one of the BERT large models and unzipping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model\n",
    "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "# unzip model\n",
    "!unzip wwm_uncased_L-24_H-1024_A-16.zip\n",
    "# start service with both GPUs available - do this another terminal\n",
    "# bert-serving-start -model_dir wwm_uncased_L-24_H-1024_A-16 -num_worker=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "import faiss\n",
    "import pickle\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "DATA_PATH = '/data/'\n",
    "d = 2014\n",
    "bc = BertClient(check_length=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use faiss to create the GPU accelerated index. Here, we first create a CPU index and then convert it to the GPU counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpus = faiss.get_num_gpus()\n",
    "print(\"number of GPUs:\", ngpus)\n",
    "cpu_index = faiss.IndexFlatL2(d)\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(  # build the index\n",
    "  cpu_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after all the touting of GPU parallelization and acceleration, I write a for loop and just process the content with brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for f in os.listdir(DATA_PATH):\n",
    "    ## read data\n",
    "    df = cudf.read_csv(DATA_PATH+f)\n",
    "    n_abstracts = len(df)\n",
    "    text.append(df['AbstractText'])\n",
    "    \n",
    "\n",
    "    ## vectorize and index in two lines \n",
    "    vectors = bc.encode(list(df['Abstract']))\n",
    "    gpu_index.add(vectors.astype('float32'))\n",
    "    print(\"Total vectors: \", gpu_index.ntotal)\n",
    "    print(\"Total abstracts: \", len(text))\n",
    "\n",
    "    ## save index to disk if desired\n",
    "    cpu_index = faiss.index_gpu_to_cpu(gpu_index)\n",
    "    faiss.write_index(cpu_index,\"data/vector.index\")\n",
    "    print(\"Saved index to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've read the text data in, vectorized it, and passed those vectors to our fast index. All that's left is to confirm and sanity check our search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "search_vector = bc.encode([\"this is a search query about the brain and blood\"])\n",
    "distances,indicies = index.search(search_vector.astype('float32'), k=3)\n",
    "t1 = time.time()\n",
    "print(\"Search query ran in %f seconds\" % t1-t0)\n",
    "print([text[i] for i in indicies])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "sandbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
